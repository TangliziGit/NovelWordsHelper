{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:06<00:00, 17.90it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "get word list from wordlist/COCA/*\n",
    "the corpus is from https://www.corpusdata.org/formats.asp\n",
    "\n",
    "wordlist['States']=='state'\n",
    "'''\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from chardet.universaldetector import UniversalDetector\n",
    "\n",
    "check_string=\"0123456789,!@#$%^*()-=+_[]{}|:;<>?\"\n",
    "wordlist={}\n",
    "\n",
    "detector = UniversalDetector()\n",
    "def get_encoding(filename: str):\n",
    "    detector.reset()\n",
    "    for line in open(filename, 'rb'):\n",
    "        detector.feed(line)\n",
    "        if detector.done: break\n",
    "    detector.close()\n",
    "    return detector.result['encoding']\n",
    "\n",
    "def get_wordlist_filepath():\n",
    "    file_path=[]\n",
    "    for dname in os.listdir('wordlist'):\n",
    "        if '.' in dname: continue\n",
    "        dpath=os.path.join('wordlist', dname)\n",
    "        for fname in os.listdir(dpath):\n",
    "            file_path.append(os.path.join(dpath, fname))\n",
    "    return file_path\n",
    "\n",
    "def check_word_validate(word: str) -> bool:\n",
    "    for c in check_string:\n",
    "        if c in word: return False\n",
    "    return True\n",
    "        \n",
    "for fpath in tqdm(get_wordlist_filepath()):\n",
    "    words=open(fpath, 'r', encoding=get_encoding(fpath)).read()\n",
    "    words=words[words.find('\\n')+1:]\n",
    "    words=[x.split('\\t') for x in words.split('\\n')]\n",
    "    \n",
    "    # data sample:\n",
    "    # 'States' 'state' 'PoS'\n",
    "    for w in words:\n",
    "        if len(w)<2: continue\n",
    "        if len(w[0])<3 or len(w[0])<3: continue\n",
    "        if not check_word_validate(w[0]): continue\n",
    "        \n",
    "        w[0]=w[0].lower()\n",
    "        wordlist[w[0]]=w[1]\n",
    "        wordlist[w[1]]=w[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "get word frequence\n",
    "'''\n",
    "\n",
    "word_freq={}\n",
    "wordset=set(wordlist.keys())\n",
    "\n",
    "def get_sample_filepath():\n",
    "    file_path=[]\n",
    "    for fname in os.listdir('sample'):\n",
    "        file_path.append(os.path.join('sample', fname))\n",
    "    return file_path\n",
    "\n",
    "for fpath in tqdm(get_sample_filepath()):\n",
    "    text=open(fpath, 'r', encoding=get_encoding(fpath)).read().lower()\n",
    "    for w in wordset:\n",
    "        ow=wordlist[w]\n",
    "        if ow not in word_freq:\n",
    "            word_freq[ow]=0\n",
    "        word_freq[ow]+=text.count(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "del word_freq['']\n",
    "pkl.dump(wordlist, open('data/wordlist.pkl', 'wb'))\n",
    "pkl.dump(word_freq, open('data/word_freq.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "info={\n",
    "    'total_word_count': reduce(lambda a, b: a+b, word_freq.values()),\n",
    "    'word_kind_count': len(word_freq),\n",
    "}\n",
    "\n",
    "pkl.dump(info, open('data/info.pkl', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "get chinese middle school words, used to filter easy words\n",
    "'''\n",
    "\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup as BS\n",
    "from tqdm import tqdm\n",
    "import pickle as pkl\n",
    "\n",
    "url='https://www.koolearn.com/dict/tag_%d_%d.html'\n",
    "\n",
    "def getWordsByUrl(url: str) -> list:\n",
    "    html=req.get(url)\n",
    "    soup=BS(html.content, 'lxml')\n",
    "\n",
    "    words=soup.find_all('div', class_='left-content')[0] \\\n",
    "        .find_all('a', class_='word')\n",
    "    words=[x.text for x in words]\n",
    "    return words\n",
    "\n",
    "words=[]\n",
    "for page in tqdm(range(423, 448+1)):\n",
    "    detail=0\n",
    "    while True:\n",
    "        detail+=1\n",
    "        new_words=getWordsByUrl(url%(page, detail))\n",
    "        if len(new_words)==0:\n",
    "            break\n",
    "        words+=new_words\n",
    "\n",
    "print(\"Done, total\", len(words), \"words\")\n",
    "words=list(set(words))\n",
    "pkl.dump(words, open('data/middle_school_words.pkl', 'wb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}